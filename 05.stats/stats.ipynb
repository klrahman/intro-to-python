{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ee75a39-5647-4cb8-843a-b6b27e7c277e",
   "metadata": {},
   "source": [
    "# basic statistical analysis with python\n",
    "\n",
    "In this exercise, we'll take a look at some basic statistical analysis with python - starting with using python and `pandas` to calculate descriptive statistics for our datasets, before moving on to look at a few common examples of hypothesis tests using `pingouin`.\n",
    " \n",
    "## data\n",
    "\n",
    "The data used in this exercise are the historic meteorological observations from the [Armagh Observatory](https://www.metoffice.gov.uk/weather/learn-about/how-forecasts-are-made/observations/recording-observations-for-over-100-years) (1853-present), the Oxford Observatory (1853-present), the Southampton Observatory (1855-2000), and Stornoway Airport (1873-present), downloaded from the [UK Met Office](https://www.metoffice.gov.uk/research/climate/maps-and-data/historic-station-data) that we used in previous exercises. I have copied the **combined_stations.csv** data into this folder - this is the same file that you created in the process of working through the \"pandas\" exercise.\n",
    "\n",
    "\n",
    "## loading libraries\n",
    "\n",
    "First, we'll import the various packages that we will be using here:\n",
    "\n",
    "- [pandas](https://pandas.pydata.org/), for reading the data from a file;\n",
    "- [seaborn](https://seaborn.pydata.org/), for plotting the data;\n",
    "- [pingouin](https://pingouin-stats.org/), for the statistical tests that we'll use;\n",
    "- [pathlib](https://docs.python.org/3/library/pathlib.html), for working with filesystem paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b612f506-1d23-45be-90b8-976790527390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pingouin as pg\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5484420-5eb9-48d1-9726-6b0031c6ba12",
   "metadata": {},
   "source": [
    "Next, we'll use `pd.read_csv()` to load the combined station data. We'll also use the `parse_dates` argument to tell `pandas` to read the `date` column as a date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168d9fff-eb8f-4a7d-b2a9-2e19d19522bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data = pd.read_csv(Path('data', 'combined_stations.csv'), parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87de769-1790-45e6-9fdf-e3c4d4db6eb3",
   "metadata": {},
   "source": [
    "## descriptive statistics\n",
    "\n",
    "Before diving into statistical tests, we'll spend a little bit of time expanding on calculating *descriptive* statistics using `pandas`. We have seen a little bit of this already, using `.groupby()` and `.mean()` to calculate the mean value of `rain` for each station.\n",
    "\n",
    "### describing variables using .describe()\n",
    "\n",
    "First, we'll have a look at `.describe()` ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html)), which provides a summary of each of the (numeric) columns in the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de60b3f6-dfe4-42bf-b981-d1151294b0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe18a6db-5e62-4683-83d5-140bce6e6093",
   "metadata": {},
   "source": [
    "In the output above, we can see the count (**count**) minimum (**min**), 1st quartile (**25%**), median (**50%**), mean (**mean**), 3rd quartile (**75%**), maximum (**max**), and standard deviation (**std**) values of each numeric variable.\n",
    "\n",
    "With this, we can quickly see where we might have errors in our data - for example, if we have non-physical or nonsense values in our variables. When first getting started with a dataset, it can be a good idea to check over the dataset using `.describe()`.\n",
    "\n",
    "### using .describe() to summarize groups\n",
    "\n",
    "What if we wanted to get a summary based on some grouping - for example, for each station? We could use `filter()` to create an object for each value of `station`, then call `summary()` on each of these objects in turn.\n",
    "\n",
    "Not surprisingly, however, there is an easier way, using `split()` ([documentation](https://rdrr.io/r/base/split.html)) and `map()` ([documentation](https://purrr.tidyverse.org/reference/map.html)). First, `split()` divides the table into separate tables based on some grouping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d423d1-af9a-44fe-9cae-82d6030ec2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data.groupby('station').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573e3156-8039-41bf-a919-295316648c8b",
   "metadata": {},
   "source": [
    "On its own, this output isn't all that readable - the summary statistics are put into individual columns, which means that the table is very wide. Let's see how we can re-arrange this so that we have a `dict()` of **DataFrames**, one for each station. First, we'll assign the output of `.describe()` to a variable, `group_summary`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1193516b-868b-4c72-8de5-5103bf4f0544",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_summary = station_data.groupby('station').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513dceb6-9d93-4f99-bba0-88eb94c4ff6c",
   "metadata": {},
   "source": [
    "Next, we'll iterate over the stations to work with each row of the table in turn. First, though, let's look at how the column names are organized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785b5b9c-1999-4002-bfce-4ed298dd814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_summary.columns # show the column names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b36eedd-cadb-4fd2-b64e-f97975760769",
   "metadata": {},
   "source": [
    "This is an example of a **MultiIndex** ([documentation](https://pandas.pydata.org/docs/reference/api/pandas.MultiIndex.html)) - a multi-level index object, similar to what we have seen previously for rows. Before beginning the `for` loop below, we use `columns.unique()` ([documentation](https://pandas.pydata.org/docs/reference/api/pandas.Index.unique.html)) to get the unique first-level names from the columns (i.e., the variable names from the original **DataFrame**). \n",
    "\n",
    "Inside of the `for` loop, we first select the row corresponding to each station using `.loc`. Have a look at this line:\n",
    "\n",
    "```python\n",
    "reshaped = pd.concat([this_summary[ind] for ind in columns], axis=1)\n",
    "\n",
    "```\n",
    "\n",
    "This uses something called [list comprehension](https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions) to quickly create a list of objects. It is effectively the same as writing something like:\n",
    "\n",
    "```python\n",
    "out_list = []\n",
    "for ind in columns:\n",
    "    out_list.append(this_summary[ind])\n",
    "\n",
    "reshaped = pd.concat(out_list, axis=1)\n",
    "\n",
    "```\n",
    "\n",
    "Using list comprehension helps make the code more concise and readable - it's a very handy tool for creating lists with iteration. In addition to list comprehension, python also has [dict comprehension](https://docs.python.org/3/tutorial/datastructures.html#dictionaries) - we won't use this here, but it works in a very similar way to list comprehension.\n",
    "\n",
    "Once we have reshaped the row (the **Series**) into a **DataFrame**, we assign the column names, before using `.append()` to add the reshaped table to a **list**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d437e1-0c85-41c7-9d1a-914f2d6d7c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = group_summary.index.unique() # get the unique values of station from the table\n",
    "columns = group_summary.columns.unique(level=0) # get the unique names of the columns from the first level (level 0)\n",
    "\n",
    "combined_stats = [] # initialize an empty list\n",
    "\n",
    "for station in stations:\n",
    "    this_summary = group_summary.loc[station] # get the row corresponding to this station\n",
    "    \n",
    "    reshaped = pd.concat([this_summary[ind] for ind in columns], axis=1) # use list comprehension to reshape the table\n",
    "    reshaped.columns = columns # set the column names\n",
    "    combined_stats.append(reshaped) # add the reshaped table to the list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815551ca-e1f6-4981-a473-768cd2abbf96",
   "metadata": {},
   "source": [
    "Finally, we'll use the built-in function `zip()` to get pairs of station names (from `station`) and **DataFrame**s (from `combined_stats`), then pass this to `dict()` to create a dictionary of station name/**DataFrame** key/value pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ccd5f4-bc76-4eeb-bb97-7a2e43f28991",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_dict = dict(zip(stations, combined_stats)) # create a dict of station name, dataframe pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8c44d7-d040-43c5-83a7-7d4b6e99d014",
   "metadata": {},
   "source": [
    "To check that this worked, let's look at the summary data for Oxford:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066d4626-bfbc-461d-a1e6-302fbf44ff08",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_dict['oxford'] # show the summary data for oxford"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961d5f24-6235-422b-8b30-f5a7c892d4a2",
   "metadata": {},
   "source": [
    "### using built-in functions for descriptive statistics\n",
    "\n",
    "This is helpful, but sometimes we want to calculate other descriptive statistics, or use the values of descriptive statistics in our code. `pandas` has a number of built-in functions for this - we have already seen `.mean()` ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mean.html)), for calculating the arithmetic mean of each column of a **DataFrame**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba35cfe1-fdee-44df-9eae-7fdd9926fb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data.mean(numeric_only=True) # calculate the mean for each numeric column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83cc057-9e58-430b-ab43-9f62070d8610",
   "metadata": {},
   "source": [
    "**Series** objects (columns/rows) also have `.mean()` ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.mean.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f03fc3-119b-408f-a130-4ad61a15e72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data['rain'].mean() # calculate the mean of the rain column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87f5186-0c0e-4949-b88f-065b0a135217",
   "metadata": {},
   "source": [
    "We can calculate the median of the columns of a **DataFrame** (or a **Series**) using `.median()` ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.median.html)): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82f1df9-8195-477d-88a9-caedff220172",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data.median(numeric_only=True) # calculate the median of each numeric column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dbde86-4d81-4634-9a68-44c421eda65e",
   "metadata": {},
   "source": [
    "To calculate the variance of the columns of a **DataFrame** (or a **Series**), use `.var()` ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.var.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b1ee8d-a776-4b1a-971e-fbdf4f91b3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data.var(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97bcbb4-2c2e-4c36-82a0-3541f354132e",
   "metadata": {},
   "source": [
    "and for the standard deviation, `.std()` ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.std.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a926358-3b9d-44a7-a355-02618bee8e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data.std(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff8c916-0857-41b6-be4f-aa8c87b599d4",
   "metadata": {},
   "source": [
    "`pandas` doesn't have a built-in function for the inter-quartile range (IQR), but we can easily calculate it ourselves using `.quantile()` ([documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.quantile.html)) to calculte the 3rd quantile and the 1st quantile and subtracting the outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25299fc2-c407-4654-ac31-328377a93517",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data.quantile(0.75, numeric_only=True) - station_data.quantile(0.25, numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab70444-f7bd-4651-8a21-51579a7b0195",
   "metadata": {},
   "source": [
    "Finally, we can also calculate the sum of each column of a **DataFrame** (or a **Series**) using `.sum()` ([documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sum.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4859f461-317a-4fbb-b968-92c112ed6bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data.sum(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da222bf-46aa-478f-af6f-da63128411ee",
   "metadata": {},
   "source": [
    "These are far from the only methods available, but they are some of the most common. For a full list, check the `pandas` [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) under **Methods**.\n",
    "\n",
    "### with .groupby()\n",
    "\n",
    "As we have seen, the output of `.groupby()` is a special type of **DataFrame**, and it inherits almost all of the methods for calculating summary statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3ba8bc-db0a-4746-a520-11a0f3fefa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data.groupby('station').mean(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca075d9-1628-4d30-b144-d0041158b949",
   "metadata": {},
   "source": [
    "## statistical tests\n",
    "\n",
    "In addition to descriptive statistics, we can use python for *inferential statistics* - for example, for hypothesis testing. In the remainder of the exercise, we'll look at a few examples of some common statistical tests and how to perform these in python, using `statsmodels`. Please note that these examples are far from exhaustive - if you're looking for a specific hypothesis test, there's a good chance someone has programmed it into python, either as part of the `statsmodels` package ([documentation](https://www.statsmodels.org/stable/index.html)), or as part of `scipy.stats` ([documentation](https://docs.scipy.org/doc/scipy/reference/stats.html)), or as part of an additional package that you can install. You should be able to find what you need with a quick internet search.\n",
    "\n",
    "### independent samples student's *t*-test\n",
    "\n",
    "For a start, let's test the hypothesis that Stornoway Airport gets more rain than Armagh. If we first have a look at a box plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32d9eb3-762c-49e3-b9d0-0135c25473b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = station_data.loc[station_data['station'].isin(['stornoway', 'armagh'])] # select stornoway airport and armagh data\n",
    "\n",
    "sns.boxplot(data=selected, x='station', y='rain')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8e09cb-a47d-4ffd-a712-b74c5ba16d9c",
   "metadata": {},
   "source": [
    "It does look like Stornoway Airport does get more rain, on average, than Armagh.\n",
    "\n",
    "To run Student's *t*-test using `pingouin`, we use `pg.ttest()` ([documentation](https://pingouin-stats.org/build/html/generated/pingouin.ttest.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb4a065-6f27-4e75-813e-baeec208bd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "armagh_rain = selected.loc[selected['station'] == 'armagh', 'rain'].dropna().sample(n=30) # take a sample of 30 rain observations from armagh\n",
    "stornoway_rain = selected.loc[selected['station'] == 'stornoway', 'rain'].dropna().sample(n=30) # take a sample of 30 rain observations from stornoway\n",
    "\n",
    "# test whether stornoway_rain.mean() > armagh_rain.mean() at the 99% confidence level\n",
    "rain_comp = pg.ttest(stornoway_rain, armagh_rain, alternative='greater', confidence=0.99)\n",
    "\n",
    "rain_comp # show the results dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414688c1-4d8e-4747-91a0-1ffeab388313",
   "metadata": {},
   "source": [
    "The output of `pg.ttest()` is a **DataFrame** that gives us the following:\n",
    "\n",
    "- `T`: the value of the *t*-statistic;\n",
    "- `dof`: the degrees of freedom;\n",
    "- `alternative`: the hypothesis that was used;\n",
    "- `p-val`: the *p*-value of the *t*-statistic;\n",
    "- `CI{confidence}%`: the confidence interval for the chosen significance level;\n",
    "- `cohen-d`: Cohen's d effect size;\n",
    "- `BF10`: the Bayes Factor of the alternative hypothesis;\n",
    "- `power`: the achieved power of the test\n",
    "\n",
    "Now, let's look at an example of a one-sample *t*-test, to see if we can determine whether the mean of a small sample of summer temperatures provides a good estimate of the mean of all summer temperatures measured at Oxford.\n",
    "\n",
    "First, we'll select all of the summer values of `tmax` recorded at Oxford, then calculate the mean value of these temperatures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8feec98-1255-4ef4-9168-76f5a7ffa587",
   "metadata": {},
   "outputs": [],
   "source": [
    "oxford_summer_tmax = station_data.query('station == \"oxford\" & season == \"summer\"')['tmax']\n",
    "\n",
    "print(f\"Oxford Mean Summer Tmax: {oxford_summer_tmax.mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9f3e53-4e1a-48ae-aff3-37fcbe7ce05f",
   "metadata": {},
   "source": [
    "So the mean summer temperature measured in Oxford between 1853-2022 is 21.1Â°C - now, let's take a random sample of 30 temperatures using `.sample()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622cf391-e88e-427a-8419-55356023a68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tmax = oxford_summer_tmax.sample(n=30) # select a random sample of 30 values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324596d3-ac37-4e59-b0bb-399835236fd5",
   "metadata": {},
   "source": [
    "Once again, we use `pg.ttest()` to conduct the test. Because `oxford_summer_tmax` is a single value, the function knows that this is a one-sample test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75962b2-c7b6-4f42-b59b-d7192d750a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test whether sample_tmax.mean() is not equal to oxford_summer_tmax at the 95% confidence level\n",
    "tmax_comp = pg.ttest(sample_tmax, oxford_summer_tmax, alternative='two-sided', confidence=0.95)\n",
    "\n",
    "tmax_comp # show the results dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bebf67b-335a-4728-a4fa-070d7a535ace",
   "metadata": {},
   "source": [
    "Based on this, we can't conclude that our sample mean is significantly different from the mean of all of the summer values of `tmax` recorded at Oxford. \n",
    "\n",
    "### non-parametric tests\n",
    "\n",
    "We can also conduct non-parametric hypothesis tests using `pingouin`. The example we will look at is the one- or two-sample Wilcoxon tests, using `pg.wilcoxon()` ([documentation](https://pingouin-stats.org/build/html/generated/pingouin.wilcoxon.html)). Let's start by looking at the Wilcoxon Rank Sum test, which is analogous to the independent sample *t*-test. For this, we'll use the same data that we did before, again testing whether Stornoway Airport gets more rainfall, on average, than Armagh:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c468f680-4690-440e-9f11-0b586971c7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test whether mean(stornoway.rain) > mean(armagh.rain)\n",
    "pg.wilcoxon(stornoway_rain, armagh_rain, alternative='greater')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3936ed-4397-45f2-b8b7-3489d991081a",
   "metadata": {},
   "source": [
    "The output table has the following columns:\n",
    "\n",
    "- `W-val`: the W-statistic of the test;\n",
    "- `alternative`: the alternative hypothesis uesd;\n",
    "- `p-val`: the *p*-value of the W-statistic;\n",
    "- `RBC`: the matched pairs rank-biserial correlation (i.e., the effect size);\n",
    "- `CLES`: the common language effect size.\n",
    "\n",
    "From this, we can again conclude that Stornoway Airport does get more rainfall, on average, than Armagh.\n",
    "\n",
    "### analysis of variance\n",
    "\n",
    "Finally, we'll see how we can set up and interpret an analysis of variance test. In this example, we'll only look at data from Armagh, Oxford, and Stornoway Airport, because the Southampton time series ends in 1999. We'll first calculate the annually-averaged values of the meteorological variables, using `.groupby()` and `.mean()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809208d4-a0fb-45da-882a-ec0586f38a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_average = station_data.loc[station_data['station'].isin(['armagh', 'oxford', 'stornoway'])] \\\n",
    "    .groupby('year') \\\n",
    "    .mean(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6660ff21-bf75-4c43-829f-b22d438a5426",
   "metadata": {},
   "source": [
    "Then, we'll add a new variable, `period`, to divide the observations into three different 50-year periods: 1871-1920, 1921-1970, and 1971-2020. To do this, we'll use `pd.cut()` ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.cut.html)). To do this, we first have to define the labels (`periods`) and category boundaries (`bins`). Then, we use `.dropna()` to remove any rows where `period` has a `NaN` value (i.e., is outside of the range 1871-2020):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01c5dd8-e1c9-49c1-b0eb-c756466f8b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "periods = ['1871-1920', '1921-1970', '1971-2020'] # make a list of period names\n",
    "bins = [1870, 1920, 1970, 2020] # make a list of period boundaries - must be 1 longer than the names\n",
    "\n",
    "annual_average['period'] = pd.cut(annual_average.index, bins, labels=periods) # assign a value to period, using the boundaries and labels above\n",
    "\n",
    "annual_average.dropna(subset=['period'], inplace=True) # drop any rows where period is NaN\n",
    "\n",
    "annual_average # show the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff83c7f-daca-4ec0-bf6f-46a74f3ae274",
   "metadata": {},
   "source": [
    "Before running the test, let's make a box plot that shows the distribution of `tmax` values among the three periods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987de4d3-0d04-4068-8e58-8f2f2d888d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=annual_average, x='period', y='tmax') # make a box plot of tmax, grouped by period"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adfb63f-aa45-4cc4-8da7-7a93a4a78120",
   "metadata": {},
   "source": [
    "From this, it certainly appears as though there is a difference in the mean value of `tmax` between the three periods. To formally test this, we'll use `aov()` ([documentation](https://rdrr.io/r/stats/aov.html)).\n",
    "\n",
    "To run the one-way ANOVA test, we use `pg.anova()` ([documentation](https://pingouin-stats.org/build/html/generated/pingouin.anova.html)). Using this, we can pass our `annual_average` **DataFrame** using the `data` argument, and specify `tmax` as the dependent variable (`dv`), and `period` as the grouping variable (`between`). We'll also look at the detailed test output (`detailed=True`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e7cd23-ec95-4bce-8c24-5f31a849e0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "aov = pg.anova(dv='tmax', between='period', data=annual_average, detailed=True) # run one-way anova for differences of tmax between periods\n",
    "\n",
    "aov.round(3) # round the output to 3 decimal places"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a783f4-c68c-4937-9e88-0424fcb2bd3b",
   "metadata": {},
   "source": [
    "The table output has the following columns across two rows:\n",
    "\n",
    "- `Source`: the factor names;\n",
    "- `SS`: the sums of squares;\n",
    "- `DF`: the degrees of freedom;\n",
    "- `MS`: the mean squares;\n",
    "- `F`: the value of the *F*-statistic;\n",
    "- `p-unc`: the uncorrected *p*-values;\n",
    "- `np2`: the partial eta-square effect sizes\n",
    "\n",
    "From the table, we can see that there are significant differences between the groups at the 99.9% significance level. \n",
    "\n",
    "This doesn't tell us which pairs of groups are different - for this, we would need to run an additional test. As one example, we can use `pg.pairwise_tukey()` ([documentation](https://pingouin-stats.org/build/html/generated/pingouin.pairwise_tukey.html)) to compute \"Tukey's Honest Significant Difference\" between each pair of groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a6c4c9-e78d-4a7a-974e-739cd26dccde",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmax_mc = pg.pairwise_tukey(dv='tmax', between='period', data=annual_average) # run the pairwise tukey HSD post-hoc test\n",
    "\n",
    "tmax_mc.round(3) # round the output to 3 decimal places"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6e4ec4-8644-4714-a235-5026abe05d7f",
   "metadata": {},
   "source": [
    "From this, we can see the estimated difference in the means for each pair of groups (shown in `A` and `B`); the estimated mean of each group; the estimated difference between the mean of each group (`diff`); the standard error of the estimate (`se`); the *t*-statistic of the estimated difference (`T`); the corrected *p*-value of the *t*-statistic (`p-tukey`), and the effect size (by default, Hedges).\n",
    "\n",
    "Using this, we can clude that, at the 99% significance level, there is a significant difference in `tmax` between the periods 1971-2020 and 1871-2020, and between the periods 1971-2020 and 1921-1970.\n",
    "\n",
    "## exercise and next steps\n",
    "\n",
    "That's all for this exercise. To help practice your skills, try at least one of the following:\n",
    "\n",
    "- Set up and run an AOV test to compare annual total rainfall at all four stations, using data from all avaialable years. Are there significant differences between the stations? Use `pg.pairwise_tukey()` or `pg.pairwise_tests()` ([documentation](https://pingouin-stats.org/build/html/generated/pingouin.pairwise_tests.html)) to investigate further.\n",
    "- Using only observations from Armagh, set up and run a test to see if there are significant differences in rainfall based on the season.\n",
    "- Using only observations from Oxford, is there a significant difference between the values of `tmax` in the spring and the autumn at the 99.9% confidence level?\n",
    "- Using only observations from Stornoway Airport, is the value of `tmin` significantly lower in the winter, compared to the autumn?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
